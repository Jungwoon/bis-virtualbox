## Sqoop2로 MySQL 데이터 Import

### MySQL Connector jar 추가

[여기]()에서 MySQL Connector jar를 다운로드 한다. 

다운로드 받은 jar 파일을 Sqoop Server 라이브러리 디렉토리에 이동/복사한다. 라이브러리를 추가하고 서버를 재기동한다.

```
$ cp mysql-connector-java-commercial-5.1.7-bin.jar /home/hadoop/sqoop/server/lib

$ sqoop2-server stop
$ sqoop2-server start

```

다른 데이터베이스에 접속을 필요하면 해당 JDBC 드라이버를 다운로드하여 Sqoop Server 라이브러리 디렉토리에 복사한다.

### Link 생성

클라이언트(sqoop2-shell)을 실행하여 MySQL Link를 생성한다.

다음 명령어로 등록되어 있는 Connector 목록을 확인한다.

```
sqoop:000> show connector     
+----+------------------------+---------+------------------------------------------------------+----------------------+
| Id |          Name          | Version |                        Class                         | Supported Directions |
+----+------------------------+---------+------------------------------------------------------+----------------------+
| 1  | generic-jdbc-connector | 1.99.6  | org.apache.sqoop.connector.jdbc.GenericJdbcConnector | FROM/TO              |
| 2  | kite-connector         | 1.99.6  | org.apache.sqoop.connector.kite.KiteConnector        | FROM/TO              |
| 3  | hdfs-connector         | 1.99.6  | org.apache.sqoop.connector.hdfs.HdfsConnector        | FROM/TO              |
| 4  | kafka-connector        | 1.99.6  | org.apache.sqoop.connector.kafka.KafkaConnector      | TO                   |
+----+------------------------+---------+------------------------------------------------------+----------------------+

```

MySQL Link를 생성하기 위해 1번인 generic-jdbc-connector사용한다.

```
Creating link for connector with id 1
Please fill following values to create new link object
Name: Memo Link

Link configuration

JDBC Driver Class: com.mysql.jdbc.Driver
JDBC Connection String: jdbc:mysql://192.168.59.3:3306/openapi
Username: openapi
Password: **********
JDBC Connection Properties: 
There are currently 0 values in the map:
entry# protocol=tcp
There are currently 1 values in the map:
protocol = tcp
entry# 
New link was successfully created with validation status OK and persistent id 1

```

HDFS에 Link를 생성하기 위해 3번인 hdfs-connector사용한다.

```
sqoop:000> create link -c 3
Creating link for connector with id 3
Please fill following values to create new link object
Name: Demo HDFS Link

Link configuration

HDFS URI: hdfs://hadoop01:9000
Hadoop conf directory: /home/hadoop/hadoop/etc/hadoop
New link was successfully created with validation status OK and persistent id 2
```

#### Job 생성

다음 명령어로 Link 목록을 확인한다.

```
sqoop:000> show link       
+----+----------------+--------------+------------------------+---------+
| Id |      Name      | Connector Id |     Connector Name     | Enabled |
+----+----------------+--------------+------------------------+---------+
| 1  | Memo Link      | 1            | generic-jdbc-connector | true    |
| 2  | Demo HDFS Link | 3            | hdfs-connector         | true    |
+----+----------------+--------------+------------------------+---------+

```

Link1의 데이터를 Link2에 저장하는 Job을 생성한다.

```
sqoop:000> create job -f 1 -t 2
Creating job for links with from id 1 and to id 2
Please fill following values to create new job object
Name: Demo Sqoop

From database configuration

Schema name: NOTIFICATION
Table name: NOTIFICATION
Table SQL statement: 
Table column names: 
Partition column name: 
Null value allowed for the partition column: 
Boundary query: 

Incremental read

Check column: 
Last value: 

To HDFS configuration

Override null value: 
Null value: 
Output format: 
  0 : TEXT_FILE
  1 : SEQUENCE_FILE
Choose: 0
Compression format: 
  0 : NONE
  1 : DEFAULT
  2 : DEFLATE
  3 : GZIP
  4 : BZIP2
  5 : LZO
  6 : LZ4
  7 : SNAPPY
  8 : CUSTOM
Choose: 0
Custom compression format: 
Output directory: /home/sqoop/demo
Append mode: 

Throttling resources

Extractors: 
Loaders: 
New job was successfully created with validation status OK  and persistent id 1
```

다음 명령어로 생성한 Job을 확인한다.

```
sqoop:000> show job
+----+------------+----------------+--------------+---------+
| Id |    Name    | From Connector | To Connector | Enabled |
+----+------------+----------------+--------------+---------+
| 1  | Demo Sqoop | 1              | 3            | true    |
+----+------------+----------------+--------------+---------+

```

Job을 실행한다.

```
sqoop:000> start job -j 1

ubmission details
Job ID: 1
Server URL: http://localhost:12000/sqoop/
Created by: hadoop
Creation date: 2016-06-01 13:38:32 KST
Lastly updated by: hadoop
External ID: job_1464673471192_0001
	http://hadoop01:8088/proxy/application_1464673471192_0001/
Source Connector schema: Schema{name=openapi.NOTIFICATION,columns=[
	Text{name=ID,nullable=true,type=TEXT,charSize=null},
	Text{name=NOTI_TYPE,nullable=true,type=TEXT,charSize=null},
	Text{name=URL,nullable=true,type=TEXT,charSize=null},
	Text{name=PAYLOAD,nullable=true,type=TEXT,charSize=null},
	Text{name=STATUS,nullable=true,type=TEXT,charSize=null},
	FixedPoint{name=RETRY_COUNT,nullable=true,type=FIXED_POINT,byteSize=4,signed=true},
	Text{name=RETRY,nullable=true,type=TEXT,charSize=null},
	Date{name=NEXT_TRY_AT,nullable=true,type=DATE_TIME,hasFraction=true,hasTimezone=false},
	Date{name=CREATED_AT,nullable=true,type=DATE_TIME,hasFraction=true,hasTimezone=false},
	Date{name=UPDATED_AT,nullable=true,type=DATE_TIME,hasFraction=true,hasTimezone=false}]}
2016-06-01 13:38:32 KST: BOOTING  - Progress is not available

```

job 실행 실패시 정확한 원인 파악을 원한다면 다음 설정으로 상세 문제를 확인할 수 있다.

```
$ set option --name verbose --value true
$ start job -j 1
```


job 상태를 확인 할 수 있다.

```
sqoop:000> status job -j 1

Submission details
Job ID: 1
Server URL: http://localhost:12000/sqoop/
Created by: hadoop
Creation date: 2016-06-01 13:38:32 KST
Lastly updated by: hadoop
External ID: job_1464673471192_0001
	http://hadoop01:8088/proxy/application_1464673471192_0001/
2016-06-01 13:40:07 KST: BOOTING  - 0.00 %

```

job을 중지한다.

```
sqoop:000> status job -j 1
```




